services:
  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: airflow
      MYSQL_USER: airflow
      MYSQL_PASSWORD: airflow
    ports:
      - "3307:3306"
    volumes:
      - mysql_data:/var/lib/mysql
    command: --default-authentication-plugin=mysql_native_password
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  airflow-init:
    build: .
    depends_on:
      - mysql
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor,airflow.providers.edge3.executors.EdgeExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://airflow:airflow@mysql:3306/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: redis://redis:6379/1
      AIRFLOW__CORE__FERNET_KEY: '1yvsWgx9agDOaUB69phwxr7mVEAWKnq-WwEmjk7jKw8='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: 2
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 1
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 16
      # ✅ 모든 컴포넌트에 내부 키 통일
      AIRFLOW__CORE__INTERNAL_API_SECRET_KEY: 'my-super-secret-key'
      AIRFLOW__API_AUTH__JWT_SECRET: 'my-super-secret-key'
    command: >
      bash -c "
        sleep 30 &&
        airflow db migrate
      "
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./filestorage:/opt/airflow/filestorage
  
  airflow-webserver:
    build: .
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor #,airflow.providers.edge3.executors.EdgeExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://airflow:airflow@mysql:3306/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: redis://redis:6379/1
      AIRFLOW__CORE__FERNET_KEY: '1yvsWgx9agDOaUB69phwxr7mVEAWKnq-WWEmjk7jKw8='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: 2
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 1
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 16
      # ✅ 모든 컴포넌트에 내부 키 통일
      AIRFLOW__CORE__INTERNAL_API_SECRET_KEY: 'my-super-secret-key'
      # ✅ (중요) api-server가 edge worker와 통신할 수 있도록 설정 추가
      AIRFLOW__EDGE__API_ENABLED: 'true'
      AIRFLOW__API_AUTH__JWT_SECRET: 'my-super-secret-key'
    command: >
      bash -c "
        sleep 45 &&
        airflow api-server
      "
    ports:
      - "8080:8080" # API 서버 포트
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./filestorage:/opt/airflow/filestorage

  airflow-scheduler:
    build: .
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor,airflow.providers.edge3.executors.EdgeExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://airflow:airflow@mysql:3306/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: redis://redis:6379/1
      AIRFLOW__CORE__FERNET_KEY: '1yvsWgx9agDOaUB69phwxr7mVEAWKnq-WWEmjk7jKw8='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
      AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: 2
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 1
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 16
      # ✅ 모든 컴포넌트에 내부 키 통일
      AIRFLOW__CORE__INTERNAL_API_SECRET_KEY: 'my-super-secret-key'
      AIRFLOW__API_AUTH__JWT_SECRET: 'my-super-secret-key'
    command: >
      bash -c "
        sleep 45 &&
        airflow scheduler
      "
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./filestorage:/opt/airflow/filestorage

  airflow-dag-processor:
    build: .
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor,airflow.providers.edge3.executors.EdgeExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://airflow:airflow@mysql:3306/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: redis://redis:6379/1
      AIRFLOW__CORE__FERNET_KEY: '1yvsWgx9agDOaUB69phwxr7mVEAWKnq-WWEmjk7jKw8='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: 2
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 1
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 16
      AIRFLOW__CORE__INTERNAL_API_SECRET_KEY: 'my-super-secret-key'
      AIRFLOW__API_AUTH__JWT_SECRET: 'my-super-secret-key'
    command: >
      bash -c "
        sleep 45 &&
        airflow dag-processor
      "
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./filestorage:/opt/airflow/filestorage

  airflow-worker:
    build: .
    depends_on:
      - airflow-init
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor,airflow.providers.edge3.executors.EdgeExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://airflow:airflow@mysql:3306/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: redis://redis:6379/1
      AIRFLOW__CORE__FERNET_KEY: '1yvsWgx9agDOaUB69phwxr7mVEAWKnq-WWEmjk7jKw8='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: http://airflow-webserver:8080/execution/
      # ✅ 모든 컴포넌트에 내부 키 통일
      AIRFLOW__CORE__INTERNAL_API_SECRET_KEY: 'my-super-secret-key'
      AIRFLOW__API_AUTH__JWT_SECRET: 'my-super-secret-key'
    command: >
      bash -c "
        sleep 60 &&
        airflow celery worker
      "
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./filestorage:/opt/airflow/filestorage

  # 다른 서버 내의 EdgeExecutor
  airflow-edge-worker:
    build: .
    restart: always
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./filestorage:/opt/airflow/filestorage
    environment:
      # ✅ 모든 컴포넌트에 내부 키 통일
      AIRFLOW__CORE__INTERNAL_API_SECRET_KEY: 'my-super-secret-key'
      AIRFLOW__CORE__EXECUTOR: airflow.providers.edge3.executors.EdgeExecutor
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: http://airflow-webserver:8080/execution/
      AIRFLOW__EDGE__API_URL: http://airflow-webserver:8080/edge_worker/v1/rpcapi
      AIRFLOW__EDGE__API_ENABLED: 'true'
      AIRFLOW__API_AUTH__JWT_SECRET: 'my-super-secret-key'
    command: >
      bash -c "
        sleep 60 &&
        airflow edge worker --queue edge_queue
      "

volumes:
  mysql_data: